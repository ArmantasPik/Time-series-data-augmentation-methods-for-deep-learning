{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":7399320,"datasetId":4241531,"databundleVersionId":7490401},{"sourceType":"kernelVersion","sourceId":158140953}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kursinis. Mašininio mokymo laiko eilučių duomenų augmentacijos metodai\nProgramų sistemų studentas Armantas Pikšrys","metadata":{"id":"fjLzPOrAZZBu"}},{"cell_type":"markdown","source":"## Mount data","metadata":{"id":"DiOgyPTmZpXn"}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W4IdRJJj7KKk","outputId":"9f455c5d-c5a8-43d9-907e-4d68233fb6c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8fbuSfT7vKp","outputId":"6dde0a90-ce46-4497-f0ed-040d9b54e5c9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_devices = tf.config.experimental.list_physical_devices('GPU')\nprint(gpu_devices)\nif gpu_devices:\n    print('Using GPU')\n    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\nelse:\n    print('Using CPU')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPXcCmw45EC1","outputId":"f6c14c01-b6ab-4468-cb88-1b22aa2d9383","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import TimeDistributed, Conv1D, BatchNormalization, AveragePooling1D, Dropout, Flatten\nfrom keras.layers import LSTM, Dense\nfrom keras import layers\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"id":"Ke8Wwhv7u1iM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input\nfrom tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"id":"mK34hUSQnejb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport six\nfrom timeit import default_timer as timer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the window size\nwindow_size = 320\n\n# Directory where your files are located\ndata_dir = \"/kaggle/input/weight-data-kilos\"  # Replace with the actual directory path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize empty lists for signals and labels\nsignals = []\nlabels = []","metadata":{"id":"fqMJPTMPfTMv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate over files in the directory\nfor filename in os.listdir(data_dir):\n    if filename.endswith(\".csv\"):\n        file_path = os.path.join(data_dir, filename)\n        if float(filename.split(\"_\")[0]) == 5:\n\n            # Load CSV data\n            data = pd.read_csv(file_path)\n\n            # Create signals using a sliding window\n            num_rows = len(data)\n            step_size = 1\n\n            for i in range(0, num_rows - window_size + 1, step_size):\n                window_data = data[i:i + window_size]\n                signals.append(window_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtgWadDOPKHH","outputId":"6debb0d4-5ecb-41a8-a354-a8df1f3b2a83","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the lists to NumPy arrays\nsignals_array = np.array(signals, dtype=np.float32)","metadata":{"id":"C5UZ-MTMfhYL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal_max = signals_array.max()\nsignal_min = signals_array.min()","metadata":{"id":"gRvRWVCckdZ7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = (signals_array - signal_min) / (signal_max - signal_min)","metadata":{"id":"tiIz7ol2fr6K","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_max = data.max()\ndata_min = data.min()\n\ndata_max, data_min","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRPPmwjsfw6d","outputId":"3b44a908-5d1b-426b-8385-228962ea2d22","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signals_array.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/abudesai/timeVAE.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd timeVAE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, warnings\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n# warnings.filterwarnings('ignore') \n\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # disabling gpu usage because my cuda is corrupted, needs to be fixed. \n\nimport sys\nimport numpy as np , pandas as pd\nimport time\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom vae_dense_model import VariationalAutoencoderDense as VAE_Dense\nfrom vae_conv_model import VariationalAutoencoderConv as VAE_Conv\nfrom vae_conv_I_model import VariationalAutoencoderConvInterpretable as TimeVAE\nimport utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\nvae_type = 'timeVAE' \n\nfull_train_data = data\nN, T, D = full_train_data.shape   \nprint('data shape:', N, T, D) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# further split the training data into train and validation set - same thing done in forecasting task\nperc_of_train_used = 20     # 5, 10, 20, 100    \nvalid_perc = 0.1\nN_train = int(N * (1 - valid_perc))\nN_valid = N - N_train\n\n# Shuffle data\nnp.random.shuffle(full_train_data)\n\ntrain_data = full_train_data[:N_train]\nvalid_data = full_train_data[N_train:]   \nprint(\"train/valid shapes: \", train_data.shape, valid_data.shape)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_train_data = train_data\n\nscaled_valid_data = valid_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntime_axis = np.arange(window_size)\n# Plot all channels in a single plot\nplt.figure(figsize=(12, 6))\n\nplt.plot(time_axis, scaled_train_data[2][:, 0], label='Channel 1', color='blue')\nplt.plot(time_axis, scaled_train_data[2][:, 1], label='Channel 2', color='green')\nplt.plot(time_axis, scaled_train_data[2][:, 2], label='Channel 3', color='red')\n\nplt.title('Signal Channels')\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------------------------------------------------------------\n# instantiate the model     \n\nlatent_dim = 20\n\nif vae_type == 'vae_dense': \n    vae = VAE_Dense( seq_len=T,  feat_dim = D, latent_dim = latent_dim, hidden_layer_sizes=[200,100], )\nelif vae_type == 'vae_conv':\n    vae = VAE_Conv( seq_len=T,  feat_dim = D, latent_dim = latent_dim, hidden_layer_sizes=[100, 200] )\nelif vae_type == 'timeVAE':\n    vae = TimeVAE( seq_len=T,  feat_dim = D, latent_dim = latent_dim, hidden_layer_sizes=[50, 100, 200],        #[80, 200, 250] [50, 100, 200]\n            reconstruction_wt = 15.0,\n            # ---------------------\n            # disable following three arguments to use the model as TimeVAE_Base. Enabling will convert to Interpretable version.\n            # Also set use_residual_conn= False if you want to only have interpretable components, and no residual (non-interpretable) component. \n\n#             trend_poly=2, \n#             custom_seas = [ (6,1), (7, 1), (8,1), (9,1)] ,     # list of tuples of (num_of_seasons, len_per_season)\n#             use_scaler = True,\n\n            #---------------------------\n            use_residual_conn = True\n        )   \nelse:  raise Exception('wut')\n\n\nvae.compile(optimizer=Adam())\n# vae.summary() ; sys.exit()\n\nearly_stop_loss = 'loss'\nearly_stop_callback = EarlyStopping(monitor=early_stop_loss, min_delta = 1e-1, patience=10) \nreduceLR = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5)\n\nvae.fit(\n    scaled_train_data, \n    batch_size = 32,\n    epochs=200,\n    shuffle = True,\n    callbacks=[early_stop_callback, reduceLR],\n    verbose = 1\n)\n\n# ----------------------------------------------------------------------------------    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ----------------------------------------------------------------------------------    \n# # save model \n# model_dir = './model/'\n# file_pref = f'vae_{vae_type}_sine_perc_{perc_of_train_used}_iter_{0}_'\n# vae.save(model_dir, file_pref)\n\n# # ----------------------------------------------------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_orig_and_post_pred_sample_custom(orig, reconst, n):\n\n    fig, axs = plt.subplots(n, 2, figsize=(10,6))\n    i = 1\n    for _ in range(n):\n        rnd_idx = np.random.choice(len(orig))\n        o = orig[rnd_idx]\n        r = reconst[rnd_idx]\n\n        plt.subplot(n, 2, i)\n        plt.imshow(o, \n            # cmap='gray', \n            aspect='auto')\n        # plt.title(\"Original\")\n        i += 1\n\n        plt.subplot(n, 2, i)\n        plt.imshow(r, \n            # cmap='gray', \n            aspect='auto')\n        # plt.title(\"Sampled\")\n        i += 1\n\n    fig.suptitle(\"Originalūs ir Rekonstruoti duomenys\")\n    fig.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------------------------------------------------------------\n# visually check reconstruction \nX = scaled_train_data\n\nx_decoded = vae.predict(scaled_train_data)\nprint('x_decoded.shape', x_decoded.shape)\n\n### compare original and posterior predictive (reconstructed) samples\ndraw_orig_and_post_pred_sample_custom(X, x_decoded, n=5)\n\n\n# # Plot the prior generated samples over different areas of the latent space\nif latent_dim == 2: utils.plot_latent_space_timeseries(vae, n=8, figsize = (20, 10))\n\n# # ----------------------------------------------------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntime_axis = np.arange(window_size)\n# Plot all channels in a single plot\nplt.figure(figsize=(12, 6))\n\nplt.plot(time_axis, scaled_train_data[0][:, 0], label='Kanalas 1', color='blue')\nplt.plot(time_axis, scaled_train_data[0][:, 1], label='Kanalas 2', color='green')\nplt.plot(time_axis, scaled_train_data[0][:, 2], label='Kanalas 3', color='red')\n\nplt.title('Originalas')\nplt.xlabel('Laikas')\nplt.ylabel('Amplitudė')\nplt.legend()\n\nplt.show()\n\ntime_axis = np.arange(window_size)\n# Plot all channels in a single plot\nplt.figure(figsize=(12, 6))\n\nplt.plot(time_axis, x_decoded[0][:, 0], label='Kanalas 1', color='blue')\nplt.plot(time_axis, x_decoded[0][:, 1], label='Kanalas 2', color='green')\nplt.plot(time_axis, x_decoded[0][:, 2], label='Kanalas 3', color='red')\n\nplt.title('Rekonstrukcija')\nplt.xlabel('Laikas')\nplt.ylabel('Amplitudė')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw random prior samples\nnum_samples = N_train\n# print(\"num_samples: \", num_samples)\n\nsamples = vae.get_prior_samples(num_samples=3)\n\nfig, axs = plt.subplots(3, 1, figsize=(6,8))\nfor i in range(3):\n    s = samples[i]\n    axs[i].plot(s)    \n\nfig.suptitle(\"Generated Samples (Scaled)\")\nfig.tight_layout()\nplt.show()\n\nfig, axs = plt.subplots(3, 1, figsize=(6,8))\ni = 0\nfor i in range(3):\n    rnd_idx = np.random.choice(len(samples))\n    s = scaled_train_data[rnd_idx]\n    axs[i].plot(s)    \n    i += 1 \n\nfig.suptitle(\"Real Samples (Scaled)\")\nfig.tight_layout()\nplt.show()\n\n# inverse-transform scaling \n# samples = scaler.inverse_transform(samples)\n# print('shape of gen samples: ', samples.shape) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw random prior samples\nnum_samples = N_train\n# print(\"num_samples: \", num_samples)\n\n#metrics_samples = vae.get_prior_samples(num_samples=N_train)\nmetrics_samples = vae.get_prior_samples(num_samples=40000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_train_data.shape, metrics_samples.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# Calculate Mean Squared Error (MSE) for each dimension\nmse_per_dimension = np.mean(np.mean((scaled_train_data - metrics_samples)**2, axis=0))\nprint(f'Mean Squared Error per Dimension: {mse_per_dimension}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = metrics_samples * (signal_max - signal_min) + signal_min","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(3, 1, figsize=(6,8))\nfor i in range(3):\n    s = samples[i]\n    axs[i].plot(s)    \n\nfig.suptitle(\"Generated Samples (Scaled)\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}